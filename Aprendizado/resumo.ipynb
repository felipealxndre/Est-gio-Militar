{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conhecendo Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: requests in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "     -------------------------------------- 204.1/204.1 MB 7.7 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 14.2 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.6.0-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 12.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.10.1\n",
      "    Uninstalling sympy-1.10.1:\n",
      "      Successfully uninstalled sympy-1.10.1\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl (7.5 kB)\n",
      "Collecting tensorflow-intel==2.18.0\n",
      "  Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl (390.0 MB)\n",
      "     -------------------------------------- 390.0/390.0 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.7/133.7 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Collecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 10.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "     -------------------------------------- 71.9/71.9 kB 780.9 kB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 15.7 MB/s eta 0:00:00\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.12.1-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 21.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (63.4.1)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "     --------------------------------------- 26.4/26.4 MB 14.5 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Downloading ml_dtypes-0.4.1-cp39-cp39-win_amd64.whl (126 kB)\n",
      "     ---------------------------------------- 126.7/126.7 kB ? eta 0:00:00\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Collecting keras>=3.5.0\n",
      "  Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 27.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.37.1)\n",
      "Collecting optree\n",
      "  Downloading optree-0.14.0-cp39-cp39-win_amd64.whl (286 kB)\n",
      "     ------------------------------------- 286.1/286.1 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: namex, libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.1.24 gast-0.6.0 google-pasta-0.2.0 h5py-3.12.1 keras-3.8.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 11.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (63.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (25.1.24)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: optree in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: namex in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: rich in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Felipe 1\\anaconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4479e8eea2444b15b78edbf69a87abdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe 1\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Felipe 1\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e4083409c54d7a94e093699cb408db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17a9051ccbc4f5f88c35c68d49537dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4523037e03b479f9792725c7c4d4970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a0d45a10b245ba8c4ff6173daacb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# pipeline acessa os modelos pré-treinados\n",
    "resumidor_texto = pipeline('summarization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_exemplo = \"\"\"\n",
    "A inteligência artificial (IA) é uma área da ciência da computação que enfatiza\n",
    "a criação de máquinas inteligentes que trabalham e reagem como seres humanos.\n",
    "Algumas das atividades que os computadores com inteligência artificial são\n",
    "projetados para fazer incluem: reconhecimento de fala, aprendizado, planejamento\n",
    "e resolução de problemas. A pesquisa associada à inteligência artificial é\n",
    "altamente técnica e especializada.Os principais problemas da inteligência\n",
    "artificial incluem programação de computadores para certos traços como conhecimento,\n",
    "raciocínio, solução de problemas, percepção, aprendizado, planejamento, habilidade\n",
    "de manipular e mover objetos.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A pesquisa associada to inteligência artificial isaltamente técnica e especializada . The IAI enfatiza a criação de máquinas inteligentes that trabalham e reagem como seres humanos . Algumas das atividades que computadores com IAI sãoprojetados for fazer incluem: reconhecimento de fala, aprendizado, planejamento, resolução of problemas'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os tamanhos são os números de tokens do resumo\n",
    "resumo = resumidor_texto(texto_exemplo, max_length = 120, min_length = 70)\n",
    "resumo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A pesquisa associada to inteligência artificial isaltamente técnica e especializada . The IAI enfatiza a criação de máquinas inteligentes that trabalham e reagem como seres humanos . Algumas das atividades que computadores com IAI sãoprojetados for fazer incluem: reconhecimento de fala, aprendizado, planejamento, resolução of problemas'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo[0]['summary_text']\n",
    "# perceba que tá misturando palavras ingles e português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ingles = \"\"\"\n",
    "Artificial intelligence (AI) is a field of computer science that emphasizes the\n",
    "creation of intelligent machines that work and react like humans. Some of the\n",
    "activities that computers with artificial intelligence are designed to perform\n",
    "include speech recognition, learning, planning, and problem-solving. Research\n",
    "associated with artificial intelligence is highly technical and specialized.\n",
    "The main problems of artificial intelligence include programming computers for\n",
    "certain traits such as knowledge, reasoning, problem-solving, perception,\n",
    "learning, planning, and the ability to manipulate and move objects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Artificial intelligence (AI) is a field of computer science that emphasizes the creation of intelligent machines that work and react like humans . The main problems of artificial intelligence include programming computers for certain traits such as knowledge, reasoning, problem-solving, perception, learning, planning, and the ability to manipulate and move objects . Research with artificial intelligence is highly technical and specialized .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os tamanhos são os números de tokens\n",
    "resumo_ingles = resumidor_texto(texto_ingles, max_length = 110, min_length = 70)\n",
    "resumo_ingles[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando o pipeline para tradução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c862f4441b7b42a798c69b5401a54024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe 1\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Felipe 1\\.cache\\huggingface\\hub\\models--google-t5--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601d8d761ea64a14bd320e498477be3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/t5-base/a90903540cc02cbeb7ff9f823f1a80eb778c7e22426a0e620b01c77a5ec8f5b4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1738590932&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODU5MDkzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby90NS1iYXNlL2E5MDkwMzU0MGNjMDJjYmViN2ZmOWY4MjNmMWE4MGViNzc4YzdlMjI0MjZhMGU2MjBiMDFjNzdhNWVjOGY1YjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=BFnwEqotE0Gto7y-Feb9TMXeivzoWt53vQlbyDJ4v%7Ed6VvobctbrwKa2-3JBfYOMCJrCS1lbTCDcwOkRcMC19w83mV-qrFgerr-AUPBmrY7J-5PopzZJDqhO-54NeBCt8%7EzU%7E5x7afkG4O627nZfDv5TlNuDvEdVxyI6M95Tz61R6PsWbk2JmP9jTq7a%7Ea-kS1OpxMN04qEwLbgHCgHzUPmqkWNT2yumNEcHMNrX9-MU1ghrtPAHTFUtQOn8hQ%7EjsLidIek%7EoGk8RuYxuwIujVV6vKAvke2aT%7EMsTHX4osQ7RLXQ1ZsG%7Eh9POE04PDzjDSQSLr99H1A4OvCmi70y9w__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75750366a7d4456861921a1cf303a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  80%|#######9  | 713M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b614f930bde54d2ea668f49513d3986c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68db3df70824f9a944f9da396d66e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa008f691c241b0a8fcc28a49e1178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# esse modelo não suporta português\n",
    "tradutor_texto = pipeline('translation_en_to_fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ingles = \"\"\"\n",
    "Artificial intelligence (AI) is a field of computer science that emphasizes the\n",
    "creation of intelligent machines that work and react like humans. Some of the\n",
    "activities that computers with artificial intelligence are designed to perform\n",
    "include speech recognition, learning, planning, and problem-solving.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"L'intelligence artificielle (AI) est un domaine d'informatique qui met l'accent sur la création de machines intelligentes qui fonctionnent et réagissent comme les humains. Certaines des activités que les ordinateurs dotés d'une intelligence artificielle sont conçus pour effectuer sont la reconnaissance de la parole, l'apprentissage, la planification et la résolution de problèmes.\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traducao = tradutor_texto(texto_ingles, max_length = 400, min_length = 100)\n",
    "traducao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L'intelligence artificielle (AI) est un domaine d'informatique qui met l'accent sur la création de machines intelligentes qui fonctionnent et réagissent comme les humains. Certaines des activités que les ordinateurs dotés d'une intelligence artificielle sont conçus pour effectuer sont la reconnaissance de la parole, l'apprentissage, la planification et la résolution de problèmes.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traducao[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo de Textos\n",
    "Escolhendo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e380c0fcd18f4a58887f6e190e205194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe 1\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Felipe 1\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188f7d2368914ce89fa8550d12b1a71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805ae46596ee430ebc568a74fc6e0097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b29af59e2a4eb3af898d915a1cc704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03a2af4648b4726b3839c442dc508e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4e44abe604472f90224bd9c346ce9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# usando o facebook bart-large-cnn\n",
    "resumidor_eng = pipeline('summarization', model = 'facebook/bart-large-cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_classificacao = \"\"\"\n",
    "Classification in machine learning involves assigning labels to input data based\n",
    "on its features and is fundamental in applications like spam detection, medical\n",
    "diagnosis, and image categorization. The process includes data collection and\n",
    "preparation, feature selection, choosing an algorithm (such as logistic regression,\n",
    "decision trees, SVM, k-NN, or neural networks), training, evaluation, hyperparameter\n",
    "tuning, and deployment. Despite its power, classification faces challenges like imbalanced\n",
    "datasets, ensuring model generalization, and handling noisy data. Successful classification\n",
    "models require careful handling of these steps and challenges to effectively solve real-world problems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Classification in machine learning involves assigning labels to input data based on its features. It is fundamental in applications like spam detection, medical diagnosis, and image categorization. Despite its power, classification faces challenges like imbalanceddatasets and noisy data.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os tokens se referem ao resumo\n",
    "resumo = resumidor_eng(texto_classificacao, max_length=70, min_length=30)\n",
    "resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification in machine learning involves assigning labels to input data based on its features. It is fundamental in applications like spam detection, medical diagnosis, and image categorization. Despite its power, classification faces challenges like imbalanceddatasets and noisy data.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_powerbi_en = \"\"\"\n",
    "Power BI is a data analysis and reporting tool developed by Microsoft. Its main\n",
    "function is to transform raw data into interactive and understandable visual\n",
    "information. Power BI is widely used by companies of all sizes to improve\n",
    "decision-making and business strategies.\n",
    "\n",
    "One of the main attractions of Power BI is its ability to integrate with a\n",
    "variety of data sources, such as SQL databases, Excel files, cloud services like\n",
    "Azure and Google Analytics, among others. This allows users to centralize all\n",
    "their data on a single platform, facilitating analysis and information sharing.\n",
    "\n",
    "Power BI is composed of three main components: Power BI Desktop, Power BI\n",
    "Service, and Power BI Mobile. Power BI Desktop is a desktop application that\n",
    "allows users to create detailed reports and dashboards. Power BI Service is an\n",
    "online platform where reports and dashboards can be published and shared with\n",
    "other members of the organization. Finally, Power BI Mobile allows users to\n",
    "access their reports and dashboards from anywhere using mobile devices.\n",
    "\n",
    "One of the most powerful features of Power BI is the ability to create\n",
    "interactive visualizations. These visualizations allow users to explore data in\n",
    "various ways, identifying patterns and trends that might go unnoticed in\n",
    "traditional data tables. Additionally, Power BI offers a vast library of\n",
    "customizable visualizations, including bar charts, line charts, geographic maps,\n",
    "scatter plots, and many others.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Power BI is a data analysis and reporting tool developed by Microsoft. Its main function is to transform raw data into interactive and understandable visual information. Power BI is composed of three main components: Power BI Desktop, Power BIService, and Power BI Mobile. It is widely used by companies of all sizes to improve decision-making and business strategies. It allows users to centralize all their data on a single platform, facilitating analysis and information sharing. It offers a vast library ofcustomizable visualizations, including bar charts, line charts, geographic maps, and many others.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def carregar_modelo(modelo):\n",
    "    resumidor = pipeline('summarization', model = modelo)\n",
    "    return resumidor\n",
    "\n",
    "def resumir_texto(texto):\n",
    "    resumidor_texto = carregar_modelo('facebook/bart-large-cnn')\n",
    "    resumo = resumidor_texto(texto, max_length = 200, min_length = 100)\n",
    "    resumo_texto = resumo[0]['summary_text']\n",
    "    return resumo_texto\n",
    "\n",
    "resumir_texto(texto_powerbi_en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c734e916ca049a9aee1fbd108a1cafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe 1\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Felipe 1\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421092d6da1c475099bfaf85fa4a1a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864d189ede8c4b0093236859a2b0cfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa1c7bfcce6414484293bed2aa428eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df834f445e894b11bbffc50a880044f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e8d134e0b04796bc7da7e5b5a6e5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc98beeb15f48fcb085b214d8caafb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce9ad9d0f8d45a7bf441b030928e1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# aqui a gente precisa especificar a linguagem de origem e destino\n",
    "tradutor = pipeline('translation', model = 'facebook/nllb-200-distilled-600M', src_lang = 'eng_Latn', tgt_lang = 'por_Latn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ingles_pt = \"\"\"\n",
    "Classification is one of the fundamental tasks in machine learning, where the\n",
    "goal is to predict the category or class of a sample based on its features. This\n",
    "technique is widely used in various applications such as image recognition,\n",
    "medical diagnosis, spam filtering, sentiment analysis, and many others.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'A classificação é uma das tarefas fundamentais do aprendizado de máquina, onde o objetivo é prever a categoria ou classe de uma amostra com base em suas características. Esta técnica é amplamente utilizada em várias aplicações, como reconhecimento de imagem, diagnóstico médico, filtragem de spam, análise de sentimentos e muitas outras. '}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traducao_texto = tradutor(texto_ingles_pt, max_length = 200)\n",
    "traducao_texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto Ingles:\n",
    "\n",
    "Classification is one of the fundamental tasks in machine learning, where the\n",
    "goal is to predict the category or class of a sample based on its features. This\n",
    "technique is widely used in various applications such as image recognition,\n",
    "medical diagnosis, spam filtering, sentiment analysis, and many others.\n",
    "\n",
    "Texto Português:\n",
    "\n",
    "A classificação é uma das tarefas fundamentais do aprendizado de máquina, onde o objetivo é prever a categoria ou classe de uma amostra com base em suas características. Esta técnica é amplamente utilizada em várias aplicações, como reconhecimento de imagem, diagnóstico médico, filtragem de spam, análise de sentimentos e muitas outras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando outras linguagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_modelo_trad(nome_modelo, ling_origem, ling_final):\n",
    "    tradutor = pipeline(\"translation\", model=nome_modelo, src_lang=ling_origem, tgt_lang=ling_final)\n",
    "    return tradutor\n",
    "\n",
    "def traduzir_texto(texto, ling_origem, ling_final):\n",
    "    tradutor_texto = carregar_modelo_trad(\"facebook/nllb-200-distilled-600M\", ling_origem, ling_final)\n",
    "    traducao = tradutor_texto(texto, max_length=400)\n",
    "    traducao_texto = traducao[0]['translation_text']\n",
    "\n",
    "    return traducao_texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_espanhol = \"\"\"\n",
    "La regresión lineal es un método estadístico utilizado para modelar la relación\n",
    "entre una variable dependiente y una o más variables independientes. El objetivo\n",
    "es encontrar la mejor línea recta que se ajuste a los datos, de manera que se\n",
    "pueda predecir el valor de la variable dependiente a partir de las variables\n",
    "independientes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "codigos_linguagens = {\n",
    "    \"Acehnese (Arabic script)\": \"ace_Arab\",\n",
    "    \"Acehnese (Latin script)\": \"ace_Latn\",\n",
    "    \"Mesopotamian Arabic\": \"acm_Arab\",\n",
    "    \"Ta’izzi-Adeni Arabic\": \"acq_Arab\",\n",
    "    \"Tunisian Arabic\": \"aeb_Arab\",\n",
    "    \"Afrikaans\": \"afr_Latn\",\n",
    "    \"South Levantine Arabic\": \"ajp_Arab\",\n",
    "    \"Akan\": \"aka_Latn\",\n",
    "    \"Amharic\": \"amh_Ethi\",\n",
    "    \"North Levantine Arabic\": \"apc_Arab\",\n",
    "    \"Modern Standard Arabic\": \"arb_Arab\",\n",
    "    \"Modern Standard Arabic (Romanized)\": \"arb_Latn\",\n",
    "    \"Najdi Arabic\": \"ars_Arab\",\n",
    "    \"Moroccan Arabic\": \"ary_Arab\",\n",
    "    \"Egyptian Arabic\": \"arz_Arab\",\n",
    "    \"Assamese\": \"asm_Beng\",\n",
    "    \"Asturian\": \"ast_Latn\",\n",
    "    \"Awadhi\": \"awa_Deva\",\n",
    "    \"Central Aymara\": \"ayr_Latn\",\n",
    "    \"South Azerbaijani\": \"azb_Arab\",\n",
    "    \"North Azerbaijani\": \"azj_Latn\",\n",
    "    \"Bashkir\": \"bak_Cyrl\",\n",
    "    \"Bambara\": \"bam_Latn\",\n",
    "    \"Balinese\": \"ban_Latn\",\n",
    "    \"Belarusian\": \"bel_Cyrl\",\n",
    "    \"Bemba\": \"bem_Latn\",\n",
    "    \"Bengali\": \"ben_Beng\",\n",
    "    \"Bhojpuri\": \"bho_Deva\",\n",
    "    \"Banjar (Arabic script)\": \"bjn_Arab\",\n",
    "    \"Banjar (Latin script)\": \"bjn_Latn\",\n",
    "    \"Standard Tibetan\": \"bod_Tibt\",\n",
    "    \"Bosnian\": \"bos_Latn\",\n",
    "    \"Buginese\": \"bug_Latn\",\n",
    "    \"Bulgarian\": \"bul_Cyrl\",\n",
    "    \"Catalan\": \"cat_Latn\",\n",
    "    \"Cebuano\": \"ceb_Latn\",\n",
    "    \"Czech\": \"ces_Latn\",\n",
    "    \"Chokwe\": \"cjk_Latn\",\n",
    "    \"Central Kurdish\": \"ckb_Arab\",\n",
    "    \"Crimean Tatar\": \"crh_Latn\",\n",
    "    \"Welsh\": \"cym_Latn\",\n",
    "    \"Danish\": \"dan_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Southwestern Dinka\": \"dik_Latn\",\n",
    "    \"Dyula\": \"dyu_Latn\",\n",
    "    \"Dzongkha\": \"dzo_Tibt\",\n",
    "    \"Greek\": \"ell_Grek\",\n",
    "    \"English\": \"eng_Latn\",\n",
    "    \"Esperanto\": \"epo_Latn\",\n",
    "    \"Estonian\": \"est_Latn\",\n",
    "    \"Basque\": \"eus_Latn\",\n",
    "    \"Ewe\": \"ewe_Latn\",\n",
    "    \"Faroese\": \"fao_Latn\",\n",
    "    \"Fijian\": \"fij_Latn\",\n",
    "    \"Finnish\": \"fin_Latn\",\n",
    "    \"Fon\": \"fon_Latn\",\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"Friulian\": \"fur_Latn\",\n",
    "    \"Nigerian Fulfulde\": \"fuv_Latn\",\n",
    "    \"Scottish Gaelic\": \"gla_Latn\",\n",
    "    \"Irish\": \"gle_Latn\",\n",
    "    \"Galician\": \"glg_Latn\",\n",
    "    \"Guarani\": \"grn_Latn\",\n",
    "    \"Gujarati\": \"guj_Gujr\",\n",
    "    \"Haitian Creole\": \"hat_Latn\",\n",
    "    \"Hausa\": \"hau_Latn\",\n",
    "    \"Hebrew\": \"heb_Hebr\",\n",
    "    \"Hindi\": \"hin_Deva\",\n",
    "    \"Chhattisgarhi\": \"hne_Deva\",\n",
    "    \"Croatian\": \"hrv_Latn\",\n",
    "    \"Hungarian\": \"hun_Latn\",\n",
    "    \"Armenian\": \"hye_Armn\",\n",
    "    \"Igbo\": \"ibo_Latn\",\n",
    "    \"Ilocano\": \"ilo_Latn\",\n",
    "    \"Indonesian\": \"ind_Latn\",\n",
    "    \"Icelandic\": \"isl_Latn\",\n",
    "    \"Italian\": \"ita_Latn\",\n",
    "    \"Javanese\": \"jav_Latn\",\n",
    "    \"Japanese\": \"jpn_Jpan\",\n",
    "    \"Kabyle\": \"kab_Latn\",\n",
    "    \"Jingpho\": \"kac_Latn\",\n",
    "    \"Kamba\": \"kam_Latn\",\n",
    "    \"Kannada\": \"kan_Knda\",\n",
    "    \"Kashmiri (Arabic script)\": \"kas_Arab\",\n",
    "    \"Kashmiri (Devanagari script)\": \"kas_Deva\",\n",
    "    \"Georgian\": \"kat_Geor\",\n",
    "    \"Central Kanuri (Arabic script)\": \"knc_Arab\",\n",
    "    \"Central Kanuri (Latin script)\": \"knc_Latn\",\n",
    "    \"Kazakh\": \"kaz_Cyrl\",\n",
    "    \"Kabiyè\": \"kbp_Latn\",\n",
    "    \"Kabuverdianu\": \"kea_Latn\",\n",
    "    \"Khmer\": \"khm_Khmr\",\n",
    "    \"Kikuyu\": \"kik_Latn\",\n",
    "    \"Kinyarwanda\": \"kin_Latn\",\n",
    "    \"Kyrgyz\": \"kir_Cyrl\",\n",
    "    \"Kimbundu\": \"kmb_Latn\",\n",
    "    \"Northern Kurdish\": \"kmr_Latn\",\n",
    "    \"Kikongo\": \"kon_Latn\",\n",
    "    \"Korean\": \"kor_Hang\",\n",
    "    \"Lao\": \"lao_Laoo\",\n",
    "    \"Ligurian\": \"lij_Latn\",\n",
    "    \"Limburgish\": \"lim_Latn\",\n",
    "    \"Lingala\": \"lin_Latn\",\n",
    "    \"Lithuanian\": \"lit_Latn\",\n",
    "    \"Lombard\": \"lmo_Latn\",\n",
    "    \"Latgalian\": \"ltg_Latn\",\n",
    "    \"Luxembourgish\": \"ltz_Latn\",\n",
    "    \"Luba-Kasai\": \"lua_Latn\",\n",
    "    \"Ganda\": \"lug_Latn\",\n",
    "    \"Luo\": \"luo_Latn\",\n",
    "    \"Mizo\": \"lus_Latn\",\n",
    "    \"Standard Latvian\": \"lvs_Latn\",\n",
    "    \"Magahi\": \"mag_Deva\",\n",
    "    \"Maithili\": \"mai_Deva\",\n",
    "    \"Malayalam\": \"mal_Mlym\",\n",
    "    \"Marathi\": \"mar_Deva\",\n",
    "    \"Minangkabau (Arabic script)\": \"min_Arab\",\n",
    "    \"Minangkabau (Latin script)\": \"min_Latn\",\n",
    "    \"Macedonian\": \"mkd_Cyrl\",\n",
    "    \"Plateau Malagasy\": \"plt_Latn\",\n",
    "    \"Maltese\": \"mlt_Latn\",\n",
    "    \"Meitei (Bengali script)\": \"mni_Beng\",\n",
    "    \"Halh Mongolian\": \"khk_Cyrl\",\n",
    "    \"Mossi\": \"mos_Latn\",\n",
    "    \"Maori\": \"mri_Latn\",\n",
    "    \"Burmese\": \"mya_Mymr\",\n",
    "    \"Dutch\": \"nld_Latn\",\n",
    "    \"Norwegian Nynorsk\": \"nno_Latn\",\n",
    "    \"Norwegian Bokmål\": \"nob_Latn\",\n",
    "    \"Nepali\": \"npi_Deva\",\n",
    "    \"Northern Sotho\": \"nso_Latn\",\n",
    "    \"Nuer\": \"nus_Latn\",\n",
    "    \"Nyanja\": \"nya_Latn\",\n",
    "    \"Occitan\": \"oci_Latn\",\n",
    "    \"West Central Oromo\": \"gaz_Latn\",\n",
    "    \"Odia\": \"ory_Orya\",\n",
    "    \"Pangasinan\": \"pag_Latn\",\n",
    "    \"Eastern Panjabi\": \"pan_Guru\",\n",
    "    \"Papiamento\": \"pap_Latn\",\n",
    "    \"Western Persian\": \"pes_Arab\",\n",
    "    \"Polish\": \"pol_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Dari\": \"prs_Arab\",\n",
    "    \"Southern Pashto\": \"pbt_Arab\",\n",
    "    \"Ayacucho Quechua\": \"quy_Latn\",\n",
    "    \"Romanian\": \"ron_Latn\",\n",
    "    \"Rundi\": \"run_Latn\",\n",
    "    \"Russian\": \"rus_Cyrl\",\n",
    "    \"Sango\": \"sag_Latn\",\n",
    "    \"Sanskrit\": \"san_Deva\",\n",
    "    \"Santali\": \"sat_Olck\",\n",
    "    \"Sicilian\": \"scn_Latn\",\n",
    "    \"Shan\": \"shn_Mymr\",\n",
    "    \"Sinhala\": \"sin_Sinh\",\n",
    "    \"Slovak\": \"slk_Latn\",\n",
    "    \"Slovenian\": \"slv_Latn\",\n",
    "    \"Samoan\": \"smo_Latn\",\n",
    "    \"Shona\": \"sna_Latn\",\n",
    "    \"Sindhi\": \"snd_Arab\",\n",
    "    \"Somali\": \"som_Latn\",\n",
    "    \"Southern Sotho\": \"sot_Latn\",\n",
    "    \"Spanish\": \"spa_Latn\",\n",
    "    \"Tosk Albanian\": \"als_Latn\",\n",
    "    \"Sardinian\": \"srd_Latn\",\n",
    "    \"Serbian\": \"srp_Cyrl\",\n",
    "    \"Swati\": \"ssw_Latn\",\n",
    "    \"Sundanese\": \"sun\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regressão linear é um método estatístico usado para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes. O objetivo é encontrar a melhor linha reta que se ajuste aos dados, de modo que o valor da variável dependente possa ser predito a partir das variáveis independentes. \n"
     ]
    }
   ],
   "source": [
    "texto_esp_pt = traduzir_texto(texto_espanhol, codigos_linguagens['Spanish'], codigos_linguagens['Portuguese'])\n",
    "print(texto_esp_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regressão linear é um método estadístico usado para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes. O objetivo é encontrar a melhor linha reta que se ajuste aos dados, de modo que o valor da variável dependente possa ser predito a partir das variáveis independentes. \n"
     ]
    }
   ],
   "source": [
    "texto_frances = \"\"\"\n",
    "La visualisation de données est un processus qui consiste à représenter\n",
    "graphiquement des informations et des données. En utilisant des éléments visuels\n",
    "tels que des graphiques, des cartes, des diagrammes et des tableaux, la\n",
    "visualisation de données permet aux individus de comprendre rapidement des\n",
    "tendances, des anomalies et des modèles dans les données. Elle est essentielle\n",
    "dans le domaine de l'analyse de données, car elle transforme des données\n",
    "complexes et volumineuses en représentations visuelles plus faciles à\n",
    "interpréter.\n",
    "\"\"\"\n",
    "\n",
    "texto_fr_pt = traduzir_texto(texto_espanhol, codigos_linguagens['French'], codigos_linguagens['Portuguese'])\n",
    "\n",
    "print(texto_fr_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unindo Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_modelos_trad_resumo(modelo_resumo, modelo_trad, ling_origem, ling_final):\n",
    "  resumidor = pipeline(\"summarization\", model=modelo_resumo)\n",
    "  tradutor = pipeline(\"translation\", model=modelo_trad, src_lang=ling_origem, tgt_lang=ling_final)\n",
    "\n",
    "  return resumidor, tradutor\n",
    "\n",
    "\n",
    "def resumir_traduzir_texto_en_pt(texto):\n",
    "\n",
    "    ling_origem = \"eng_Latn\"\n",
    "    ling_final = \"por_Latn\"\n",
    "\n",
    "    modelo_resumo = \"facebook/bart-large-cnn\"\n",
    "    modelo_trad = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "    resumidor_texto, tradutor_texto = carregar_modelos_trad_resumo(modelo_resumo, modelo_trad, ling_origem, ling_final)\n",
    "\n",
    "    # Resumo\n",
    "    resumo = resumidor_texto(texto, max_length=200, min_length=100)\n",
    "    resumo_texto = resumo[0]['summary_text']\n",
    "\n",
    "    # Tradução\n",
    "    traducao = tradutor_texto(resumo_texto, max_length=400)\n",
    "    traducao_resumo_texto = traducao[0]['translation_text']\n",
    "\n",
    "    return traducao_resumo_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_nlp_eng = \"\"\"\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that\n",
    "focuses on the interaction between computers and human language. It involves\n",
    "the development of algorithms and models that enable machines to understand,\n",
    "interpret, and generate human language. NLP combines concepts from computer science,\n",
    "linguistics, and machine learning to process and analyze large amounts of natural\n",
    "language data.\n",
    "\n",
    "The ability to process and understand human language is crucial for many applications\n",
    "in today's digital world. NLP powers a wide range of technologies that we use daily, including:\n",
    "Search Engines: Understanding and processing user queries to return relevant search results.\n",
    "Voice Assistants: Enabling devices like Siri, Alexa, and Google Assistant to understand spoken\n",
    "commands and respond appropriately.\n",
    "Translation Services: Providing accurate translations between different languages, as seen in\n",
    "tools like Google Translate.\n",
    "Customer Support: Automating responses in chatbots and virtual assistants to handle customer\n",
    "inquiries efficiently.\n",
    "Text Analysis: Analyzing sentiments, extracting key information, and summarizing documents for\n",
    "better insights.\n",
    "\n",
    "NLP has a wide array of applications across various industries:\n",
    "Healthcare: NLP is used to process and analyze medical records, extract relevant information,\n",
    "and assist in clinical decision-making.\n",
    "Finance: Analyzing financial documents, news, and reports to provide insights and make informed decisions.\n",
    "Marketing: Understanding customer sentiments and feedback to tailor marketing strategies and improve\n",
    "customer experience.\n",
    "Legal: Automating the review of legal documents, contracts, and compliance reports to save time and\n",
    "reduce errors.\n",
    "Education: Enhancing educational tools, grading systems, and providing personalized learning experiences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) é um campo de inteligência artificial que se concentra na interação entre computadores e linguagem humana. A NLP combina conceitos de ciência da computação, linguística e aprendizado de máquina para processar e analisar grandes quantidades de dados de linguagem natural. A NLP alimenta uma ampla gama de tecnologias que usamos diariamente, incluindo motores de busca, assistentes de voz e chatbots. Tem uma ampla gama de aplicações em várias indústrias, incluindo saúde, finanças, educação e jurídico. A capacidade de entender e entender a linguagem humana é crucial para muitas aplicações no mundo digital de hoje.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = resumir_traduzir_texto_en_pt(texto_nlp_eng)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depois disso aqui, tem a parte da Interface que não é bizu pra mim\n",
    "\n",
    "Mesmo assim eu vou fazer aqui pra deixar de bizu pra outra oportunidade(mas aqui eu não aprendi muito, só fui seguindo o que o povo fazia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "     --------------------------------------- 18.1/18.1 MB 20.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (3.5.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (2.2.3)\n",
      "Collecting tomlkit==0.12.0\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting gradio-client==1.3.0\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "     ------------------------------------- 318.7/318.7 kB 19.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (2.3.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (2.0.1)\n",
      "Collecting ruff>=0.2.2\n",
      "  Downloading ruff-0.9.4-py3-none-win_amd64.whl (10.9 MB)\n",
      "     --------------------------------------- 10.9/10.9 MB 19.3 MB/s eta 0:00:00\n",
      "Collecting uvicorn>=0.14.0\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.3/62.3 kB ? eta 0:00:00\n",
      "Collecting python-multipart>=0.0.9\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (3.1.5)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (3.9.4)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting semantic-version~=2.0\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting fastapi<1.0\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.8/94.8 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Collecting orjson~=3.0\n",
      "  Downloading orjson-3.10.15-cp39-cp39-win_amd64.whl (133 kB)\n",
      "     -------------------------------------- 133.4/133.4 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (2.0.2)\n",
      "Collecting typer<1.0,>=0.12\n",
      "  Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio) (9.2.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from gradio-client==1.3.0->gradio) (2025.2.0)\n",
      "Collecting websockets<13.0,>=10.0\n",
      "  Downloading websockets-12.0-cp39-cp39-win_amd64.whl (124 kB)\n",
      "     -------------------------------------- 125.0/125.0 kB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<0.46.0,>=0.40.0\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.5/71.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.0.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Collecting anyio<5.0,>=3.0\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.0/96.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Installing collected packages: pydub, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, ffmpy, anyio, aiofiles, uvicorn, starlette, typer, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.11.1\n",
      "    Uninstalling tomlkit-0.11.1:\n",
      "      Successfully uninstalled tomlkit-0.11.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n",
      "Successfully installed aiofiles-23.2.1 anyio-4.8.0 fastapi-0.115.8 ffmpy-0.5.0 gradio-4.44.1 gradio-client-1.3.0 orjson-3.10.15 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.45.3 tomlkit-0.12.0 typer-0.15.1 uvicorn-0.34.0 websockets-12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.18.1 requires anyio<4,>=3.1.0, but you have anyio 4.8.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "nome_modelo = 'facebook/nllb-200-distilled-600M'\n",
    "\n",
    "tradutor = pipeline('translation', model=nome_modelo, src_lang=\"eng_Latn\", tgt_lang=\"por_Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traducao_eng_por(texto):\n",
    "      texto_traduzido = tradutor(texto, max_length=400)\n",
    "      return texto_traduzido[0][\"translation_text\"]\n",
    "\n",
    "app = gr.Interface(\n",
    "    fn=traducao_eng_por,\n",
    "    inputs= gr.Textbox(lines=5, label=\"Texto original\"),\n",
    "    outputs= gr.Textbox(lines=5, label=\"Texto traduzido\"),\n",
    "    title= \"Tradutor de texto\",\n",
    "    description= \"Envie o texto em português para traduzir para inglês\"\n",
    ")\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def traduzir_resumir_textos(texto, ling_origem, ling_final):\n",
    "\n",
    "    nome_modelo_traducao = 'facebook/nllb-200-distilled-600M'\n",
    "    nome_modelo_resumo = 'facebook/bart-large-cnn'\n",
    "\n",
    "    idioma_origem = codigos_linguagens[ling_origem]\n",
    "    idioma_final = codigos_linguagens[ling_final]\n",
    "\n",
    "    if idioma_origem == \"eng_Latn\":\n",
    "        resumidor = pipeline('summarization', model=nome_modelo_resumo)\n",
    "        resumo = resumidor(texto, max_length=200, min_length=100)[0]['summary_text']\n",
    "    else:\n",
    "        tradutor = pipeline('translation', model=nome_modelo_traducao, src_lang=idioma_origem, tgt_lang='eng_Latn')\n",
    "        texto_traduzido = tradutor(texto, max_length=512)[0]['translation_text']\n",
    "\n",
    "        resumidor = pipeline('summarization', model=nome_modelo_resumo)\n",
    "        resumo = resumidor(texto_traduzido, max_length=200, min_length=100)[0]['summary_text']\n",
    "\n",
    "    tradutor = pipeline('translation', model=nome_modelo_traducao, src_lang='eng_Latn', tgt_lang=idioma_final)\n",
    "    texto_traduzido = tradutor(texto, max_length=512)[0]['translation_text']\n",
    "\n",
    "    return texto_traduzido\n",
    "\n",
    "\n",
    "cod_idiomas = list(codigos_linguagens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = [\n",
    "    gr.Textbox(lines=5, label='Insira o texto'),\n",
    "    gr.Dropdown(cod_idiomas, value= 'English', label='Texto original'),\n",
    "    gr.Dropdown(cod_idiomas, value= 'Portuguese', label='Texto final')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "saidas = gr.Textbox(lines=5, label='Texto final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = 'Tradução e Resumo'\n",
    "\n",
    "descr = 'Esta aplicação resume e traduz o texto inserido. Modelo de tradução: facebook/nllb-200-distilled-600M. Modelo de sumarização: facebook/bart-large-cnn.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 200, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "aplicacao = gr.Interface(\n",
    "    fn = traduzir_resumir_textos,\n",
    "    inputs = entradas,\n",
    "    outputs = saidas,\n",
    "    title = titulo,\n",
    "    description = descr\n",
    ")\n",
    "\n",
    "aplicacao.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício Extra:\n",
    "### Extraindo palavras chaves de textos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estratégia 1: Usando keybert e sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "     -------------------------------------- 275.9/275.9 kB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keybert) (13.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keybert) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from keybert) (1.6.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\felipe 1\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: networkx in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.8.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe 1\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Installing collected packages: sentence-transformers, keybert\n",
      "Successfully installed keybert-0.8.5 sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'não', 'nas', 'nem', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'seu', 'seus', 'só', 'somos', 'sou', 'sua', 'suas', 'também', 'te', 'tem', 'tém', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terá', 'terão', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tu', 'tua', 'tuas', 'um', 'uma', 'você', 'vocês', 'vos']\n",
      "\n",
      "\n",
      "Palavras-chave:\n",
      "modelos nlp\n",
      "data science\n",
      "nlp llm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# esse modelo é só de exemplo pra embeddings, podemos escolher outro\n",
    "modelo_embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "kw_model = KeyBERT(modelo_embeddings)\n",
    "\n",
    "texto = \"Data Science é muito legal. Poderiamos passar o dia estudando modelos de NLP com LLM.\"\n",
    "\n",
    "stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "print(stopwords_pt)\n",
    "\n",
    "# ai vamos colocar até bigrams\n",
    "# pegando só as top 3\n",
    "keywords = kw_model.extract_keywords(texto, keyphrase_ngram_range=(1,2), stop_words=stopwords_pt, top_n= 3)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n\\nPalavras-chave:\")\n",
    "[print(kw[0]) for kw in keywords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estratégia 2: Usando NER(Reconhecimento de Entidade Nomeadas) -> E isso aqui seria o bizu no contexto médico, a gente podia explorar melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\Felipe 1\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Disease_disorder',\n",
       "  'score': np.float32(0.94586253),\n",
       "  'word': 'diabetes mellitus',\n",
       "  'start': 33,\n",
       "  'end': 50}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Esse modelo também é bom pra contexto médico:\n",
    "ner_model_med = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", grouped_entities=True)\n",
    "\n",
    "texto = \"O paciente foi diagnosticado com diabetes mellitus e está em tratamento com metformina e insulina.\"\n",
    "\n",
    "# Extração de entidades nomeadas médicas\n",
    "# ai tem que ver as entidades médicas que tem\n",
    "entidades_medicas = ner_model_med(texto)\n",
    "\n",
    "entidades_medicas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
